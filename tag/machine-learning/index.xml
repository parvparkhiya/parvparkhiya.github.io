<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Parv Parkhiya</title>
    <link>https://parvparkhiya.github.io/tag/machine-learning/</link>
      <atom:link href="https://parvparkhiya.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 27 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://parvparkhiya.github.io/media/icon_hu651937ae150e4d5d4164cf9501a55289_56927_512x512_fill_lanczos_center_3.png</url>
      <title>Machine Learning</title>
      <link>https://parvparkhiya.github.io/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Taking Out Trash</title>
      <link>https://parvparkhiya.github.io/project/trash/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://parvparkhiya.github.io/project/trash/</guid>
      <description>&lt;p&gt;Modeled picking and placing trash bin skill using manipulator arm of Locobot robotic platform as Gaussian Process (GP) to enable imitation based skill learning from single demonstration&lt;/p&gt;
&lt;p&gt;The objective of this project is to design a robot which can move a small trash can from its original place to a target place. Basically, what it should do are as follows: first, it should be able to navigate in the working environment. It should be able to detect the trash can, and move towards to it by using visual servoing. Then, it can use some grasping mechanism to carry the trash can with it and move to the target position. Finally, the robot puts down the trash can in the target area, and moves itself back to rest area. When conducting its mission, the robot should be able to hold the trash can steadily and not spill the trash out of the trash can. In addition, the robot is going to operate in the corridors in the teaching building, it should have a suitable physical size and make very little noise.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/zK49eIqJDW0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Modeling Motion of Stereotypical Dynamic Objects for Efficient Interaction</title>
      <link>https://parvparkhiya.github.io/project/dmp/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://parvparkhiya.github.io/project/dmp/</guid>
      <description>&lt;p&gt;Incorporated Dynamic Movement Primitives (DMP) approach to model stereo typical motion in data efficient manner and used that model to predict trajectory and goal location from a partially observed trajectory.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Constructing Category-Specific Models for Monocular Object-SLAM</title>
      <link>https://parvparkhiya.github.io/project/oslam/</link>
      <pubDate>Fri, 02 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://parvparkhiya.github.io/project/oslam/</guid>
      <description>&lt;p&gt;We present a new paradigm for real-time object-oriented SLAM with a monocular camera. Contrary to previous approaches, that rely on object-level models, we construct category-level models from CAD collections which are now widely available. To alleviate the need for huge amounts of labeled data, we develop a rendering pipeline that enables synthesis of large datasets from a limited amount of manually labeled data. Using data thus synthesized, we learn category-models for object deformations in 3D, as well as discriminative object features in 2D. These category models are instance-independent and aid in the design of object landmark observations that can be incorporated into a generic monocular SLAM framework. Where typical object-SLAM approaches usually solve only for object and camera poses, we also estimate object shape on-the-fly, allowing for a wide range of objects from the category to be present in the scene. Moreover, since our 2D object features are learned discriminatively, the proposed object-SLAM system succeeds in several scenarios where sparse feature-based monocular SLAM fails due to insufficient features or parallax. Also, the proposed category-models help in object instance retrieval, useful for Augmented Reality (AR) applications. We evaluate the proposed framework on multiple challenging real-world scenes and show — to the best of our knowledge — first results of an instance-independent monocular object-SLAM system and the benefits it enjoys over feature-based SLAM methods.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/jzZl4RUq5-Y&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Camera Model Identification</title>
      <link>https://parvparkhiya.github.io/project/cmi/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://parvparkhiya.github.io/project/cmi/</guid>
      <description>&lt;p&gt;We apply the algorithm proposed by Chen etal to identify camera models. The algorithm assumes that CFA pattern used by the device is GBRG. Local co-occurrence features are computed using multiple interpolation algorithms (example nearest neighbour, bilinear). A multi-class linear SVM is trained with these features and employed to classify the given image to one of the camera classes. Some observations have been made with respect to validation accuracy of the model and the results obtained on Kaggle.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Doubly Convolutional Neural Networks [Implementation]</title>
      <link>https://parvparkhiya.github.io/project/dcnn/</link>
      <pubDate>Thu, 01 Dec 2016 00:00:00 +0000</pubDate>
      <guid>https://parvparkhiya.github.io/project/dcnn/</guid>
      <description>&lt;p&gt;Parameter sharing is the major reason of success of building large models for deep neural networks. The paper proposed by Shuangfei Zhai, Yu Cheng, Weining Lu and Zhongfei Zhang (NIPS 2016) introduces the idea of Doubly Convolutional Neural Networks, which significantly improves the performance of CNN with the same number of parameters. We have implemented DCNN as a part of Statistical  Methods in AI Project.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
